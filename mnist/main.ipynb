{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3e2fb0-10f1-472f-a9f6-e2a46b376319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision, datasets, diffusers, accelerate\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size=32\n",
    "    train_batch_size = 32\n",
    "    eval_batch_size = 32\n",
    "    num_epochs = 15\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmpup_steps = 500\n",
    "    mixed_precision = 'fp16'\n",
    "    seed = 0\n",
    "    \n",
    "config = TrainingConfig()\n",
    "\n",
    "mnist_dataset = datasets.load_dataset('mnist', split='train')\n",
    "mnist_dataset.reset_format()\n",
    "\n",
    "def transform(dataset):\n",
    "    preprocess = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize(\n",
    "                (config.image_size, config.image_size)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: 2*(x-0.5)),\n",
    "        ]\n",
    "    )\n",
    "    images = [preprocess(image) for image in dataset[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "indices_zero = [i for i, datum in enumerate(mnist_dataset) if datum['label'] == 0]\n",
    "indices_one  = [i for i, datum in enumerate(mnist_dataset) if datum['label'] == 1]\n",
    "indices_one  = random.sample(indices_one, int(len(indices_zero) * 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6633bff-b1a1-44c2-aca1-3d88188866c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = indices_zero + indices_one\n",
    "mnist_dataset.set_transform(transform)\n",
    "subset = Subset(mnist_dataset, indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    subset,\n",
    "    batch_size = config.train_batch_size,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "model = diffusers.UNet2DModel(\n",
    "    sample_size=config.image_size,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128,128,256,512),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",\n",
    "        \"AttnUpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "noise_scheduler = diffusers.DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=config.learning_rate)\n",
    "\n",
    "lr_scheduler = diffusers.optimization.get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmpup_steps,\n",
    "    num_training_steps=(len(train_dataloader)*config.num_epochs),\n",
    ")\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device);\n",
    "model = diffusers.UNet2DModel.from_pretrained('data/modelstd').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d91aa9-68bc-45df-8548-59902377bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M, m = 20, .1\n",
    "tau   = lambda t: np.exp(-(t * m + t**2 * (M-m)/2)/2)\n",
    "a     = lambda t: (1-tau(t)**2)**(1/2)\n",
    "ap    = lambda t: -tau(t)/a(t) * bp(t)\n",
    "b     = lambda t: tau(t)\n",
    "bp    = lambda t: -(m+(M-m)*t)/2 * tau(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7763186-d0b7-4cbd-b750-349b8aec2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(\n",
    "        config,\n",
    "        model,\n",
    "        noise_scheduler,\n",
    "        optimizer,\n",
    "        train_dataloader,\n",
    "        lr_scheduler):\n",
    "\n",
    "    accelerator = accelerate.Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    )\n",
    "\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader),\n",
    "                            disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            batch_size = clean_images.shape[0]\n",
    "\n",
    "            ts = torch.rand((batch_size,), device=clean_images.device)\n",
    "            # For training with a non-uniform time grid, the previous line should\n",
    "            #  be changed to, e.g., the following, which puts half of the training \n",
    "            #  points in [.2, .6]\n",
    "            '''\n",
    "            ts1  = torch.rand((batch_size//8,), device=clean_images.device) * .2\n",
    "            ts2  = torch.rand((batch_size//2,), device=clean_images.device) * .2 + .2\n",
    "            ts3  = torch.rand((3*batch_size//8,), device=clean_images.device) * .6 + .4\n",
    "            ts   = torch.cat((ts1, ts2, ts3))\n",
    "            '''\n",
    "            a_ts = torch.tensor(a(ts.cpu().numpy()), device=clean_images.device)\n",
    "            b_ts = torch.tensor(b(ts.cpu().numpy()), device=clean_images.device)\n",
    "            noisy_images = a_ts[:,None,None,None] * noise + b_ts[:,None,None,None] * clean_images\n",
    "            \n",
    "            with accelerator.accumulate(model):\n",
    "                noise_pred = model(noisy_images, ts*1000)[\"sample\"]\n",
    "                loss = torch.nn.functional.mse_loss(noise_pred,noise)\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                accelerator.clip_grad_norm_(model.parameters(),1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            if step % 100 == 99:\n",
    "                progress_bar.update(100)\n",
    "                logs = {\n",
    "                    \"loss\" : loss.detach().item(),\n",
    "                    \"lr\" : lr_scheduler.get_last_lr()[0],\n",
    "                }\n",
    "                progress_bar.set_postfix(**logs)\n",
    "    \n",
    "    accelerator.unwrap_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e92f98-38ce-49c3-ad0e-56f6738e042a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
    "accelerate.notebook_launcher(train_loop, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525776e1-18b9-4f3c-b120-fb80c25935e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "cuda1 = torch.device('cuda:1')\n",
    "model.to(cuda1)\n",
    "n = 1000\n",
    "import os.path\n",
    "\n",
    "with torch.no_grad():\n",
    "    ts = np.linspace(1, 0, n+1)\n",
    "    x  = torch.randn((30,1,32,32)).to(model.device)\n",
    "    \n",
    "    for i, t in enumerate(ts[:-1]):\n",
    "        noiser = model(x, t*n)['sample']\n",
    "        bf = lambda x, t: bp(t)/b(t)*x + 2*(ap(t) - bp(t)/b(t)*a(t))*noiser\n",
    "        z  = torch.randn_like(x)\n",
    "        dt = -(ts[i+1] - ts[i])\n",
    "        x = x - bf(x, t) * dt + (-2*bp(t)/b(t) * dt) ** (1/2) * z\n",
    "        \n",
    "        if i % (n//10) == 0:\n",
    "            print(t)\n",
    "            fig, axes = plt.subplots(nrows=1, ncols=x.shape[0], figsize=(x.shape[0], 1))\n",
    "            for j in range(x.shape[0]):\n",
    "                axes[j].imshow(torchvision.transforms.ToPILImage()(x.clamp(-1, 1)[j].squeeze(0)), cmap='gray')\n",
    "                axes[j].axis('off')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefe72e-eadf-4dfc-ba6f-8193c4f2db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Turn\n",
    "'''\n",
    "Given a model using the uniform grid, we can perform the U-Turn to determine when\n",
    " the digit being 0 or 1 is decided (c.f. Figure 2 on https://arxiv.org/pdf/2412.07972).\n",
    "The code below performs the U-Turn for a variable t_0 at which the turn is made. Then,\n",
    " the generated data is saved and we used a discriminator to tell whether the digit\n",
    " belongs to the 0 or 1 class.\n",
    "'''\n",
    "ss = np.arange(.35, .9, .05)\n",
    "k = 500\n",
    "n = 1000\n",
    "device = torch.device('cuda:1')\n",
    "model.to(device)\n",
    "\n",
    "for ii in range(len(indices_one)//k):\n",
    "    for s in ss:\n",
    "        print(s)\n",
    "        x = torch.cat([a['images'] for a in Subset(mnist_dataset, indices_zero[ii*k:(ii+1)*k])]).to(device)[:, None, :, :]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ts = np.linspace(s, 0, int(n*s)+1)\n",
    "            # data  -> noise\n",
    "            for i, t in enumerate(list(reversed(ts))[:-1]):\n",
    "                noiser = model(x, t*n)['sample']\n",
    "                bf = lambda x, t: bp(t)/b(t)*x\n",
    "                z  = torch.randn_like(x)\n",
    "                dt = -(ts[i+1] - ts[i])\n",
    "                x = x + bf(x, t) * dt + (-2*bp(t)/b(t) * dt) ** (1/2) * z\n",
    "                '''\n",
    "                if i % ((n*s)//4) == 0 or int(n*s)-i < 5:\n",
    "                    l = min(20, k)\n",
    "                    fig, axes = plt.subplots(nrows=1, ncols=l, figsize=(l, 1))\n",
    "                    for j in range(l):\n",
    "                        axes[j].imshow(torchvision.transforms.ToPILImage()(x.clamp(-1, 1)[j].squeeze(0)), cmap='gray')\n",
    "                        axes[j].axis('off')\n",
    "                    plt.show()\n",
    "                '''\n",
    "                \n",
    "            # noise -> data\n",
    "            for i, t in enumerate(ts[:-1]):\n",
    "                noiser = model(x, t*n)['sample']\n",
    "                bf = lambda x, t: bp(t)/b(t)*x + 2*(ap(t) - bp(t)/b(t)*a(t))*noiser\n",
    "                z  = torch.randn_like(x)\n",
    "                dt = -(ts[i+1] - ts[i])\n",
    "                x = x - bf(x, t) * dt + (-2*bp(t)/b(t) * dt) ** (1/2) * z\n",
    "                '''\n",
    "                if i % ((n*s)//4) == 0 or int(n*s)-i < 5:\n",
    "                    l = min(20, k)\n",
    "                    fig, axes = plt.subplots(nrows=1, ncols=l, figsize=(l, 1))\n",
    "                    for j in range(l):\n",
    "                        axes[j].imshow(torchvision.transforms.ToPILImage()(x.clamp(-1, 1)[j].squeeze(0)), cmap='gray')\n",
    "                        axes[j].axis('off')\n",
    "                    plt.show()\n",
    "                '''\n",
    "\n",
    "        fn = f'new-d=0,t={int(s*100)/100}'\n",
    "        if os.path.isfile(fn):\n",
    "            xo = torch.load(fn)\n",
    "            xn = torch.cat((xo, x))\n",
    "        else:\n",
    "            xn = x    \n",
    "        torch.save(xn, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95f89b-5740-4bfc-8b14-46c76a0b4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate\n",
    "import torchvision.transforms as transforms\n",
    "k = 3\n",
    "fig, axes = plt.subplots(nrows=k, ncols=x.shape[0]//k, figsize=(x.shape[0]//k, k))\n",
    "resize_transform = transforms.Resize((28, 28))\n",
    "\n",
    "for i in range(k):\n",
    "    for j in range(x.shape[0]//k):\n",
    "        img = torchvision.transforms.ToPILImage()(x.clamp(-1, 1)[j + i * x.shape[0]//k].squeeze(0))\n",
    "        img_resized = resize_transform(img)\n",
    "        axes[i][j].imshow(img_resized, cmap='gray')\n",
    "        axes[i][j].axis('off')\n",
    "\n",
    "plt.savefig('std.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
